# üó∫Ô∏è Roadmap: Proyecto Lotus-2026

Este documento detalla la hoja de ruta estrat√©gica para el desarrollo y estabilizaci√≥n del proyecto, integrando correcciones de errores cr√≠ticos y mejoras evolutivas.

> **Leyenda de Prioridad:**
> - üî¥ **CR√çTICO:** Debe resolverse de inmediato (bloqueante o riesgo alto).
> - üü° **ALTO:** Importante para la funcionalidad y precisi√≥n core.
> - üîµ **MEDIO:** Mejoras de arquitectura, UX o rendimiento.
> - ‚ö™ **BAJO/FUTURO:** Ideas experimentales o mejoras menores.

---

## üèóÔ∏è Fase 1: Estabilizaci√≥n y Correcci√≥n de Errores (Semana 1-2)
*Objetivo: Eliminar bugs cr√≠ticos, asegurar la integridad de datos y prevenir fallos silenciosos.*

### üî¥ Seguridad y Estabilidad
- [x] **[ERR-001] Fix Race Condition en `consolidar_cola.py`**: Reemplazar loop infinito inseguro por `threading.Lock` o `portalocker` para evitar deadlocks.
- [x] **[ERR-003] Sanitizaci√≥n de Inputs**: Reemplazar `ast.literal_eval()` por `json.loads()` en `juez_implacable.py` para prevenir inyecci√≥n de c√≥digo.
- [x] **[ERR-002] Validaci√≥n de Or√°culo**: Asegurar que `OraculoNeural` tenga m√©todo `predecir()` antes de invocarlo para evitar fallos silenciosos.
- [x] **[ERR-004] Fix NaN Handling**: Reemplazar condici√≥n fr√°gil `v == v` por `pd.isna()` en `consolidar_laboratorio.py`.

### üî¥ L√≥gica de Negocio Core
- [x] **[IMP-AUD-001] Corregir Scoring RACHA**: Eliminar la curva en V invertida que asigna 100% de √©xito a 0 aciertos. Implementar curva mon√≥tona.
- [x] **[ERR-005] Consenso Robusto**: Asegurar que el loop de consenso en `bot_dreamer.py` no termine prematuramente con muestras insuficientes (<5).
- [x] **[ERR-006] Fix IndexError en Ventanas Peque√±as**: Validar tama√±o de dataset en `oraculo_neural.py` antes de acceder a √≠ndices negativos.

### üü° Calidad de Datos
- [x] **[ERR-007] Robustez de Scraper**: Aumentar timeout en `scraper_maestro.py` y manejar esperas expl√≠citas para conexiones lentas.
- [x] **[IMP-DATA-003] Backups Autom√°ticos**: Implementar copia `.bak` antes de que `juez_implacable.py` modifique `SIMULACIONES.csv`.
- [x] **[FIX-PIPE-001] Filtrado de Pipeline IA**: Asegurar que solo los juegos con nuevos sorteos activen el reentrenamiento y optimizaci√≥n.

---

## üöÄ Fase 2: Optimizaci√≥n de Inteligencia Artificial (Semana 3-4)
*Objetivo: Mejorar la precisi√≥n predictiva y reducir el overfitting detectado.*

### üü° Mejoras de Modelado (ML)
- [x] **[IMP-ML-001] Reducir Overfitting en Random Forest**: Ajustar hiperpar√°metros (`max_depth=5`, `min_samples_leaf=20`) en `oraculo_neural.py`.
- [x] **[IMP-ML-003] Optimizaci√≥n de Hiperpar√°metros**: Implementar `GridSearchCV` con `TimeSeriesSplit` para encontrar la configuraci√≥n √≥ptima autom√°ticamente.
- [x] **[IMP-ML-002] Explorar Gradient Boosting**: Integrar XGBoost o LightGBM como alternativas a Random Forest y comparar rendimiento.

### üîµ Ingenier√≠a de Caracter√≠sticas (Feature Engineering)
- [x] **[IMP-FEAT-001] An√°lisis de Rachas**: Crear features para detectar n√∫meros "calientes" (frecuentes recientes) y "fr√≠os".
- [x] **[IMP-FEAT-003] Correlaci√≥n Posicional**: Analizar si el valor de una bola influye en la paridad o terminaci√≥n de la siguiente.

### üîµ Validaci√≥n
- [x] **[IMP-ML-008] Validaci√≥n Cruzada Temporal**: Implementar `TimeSeriesSplit` (5 folds) en lugar de un simple split 80/20 para m√©tricas m√°s realistas.

---

## üß† Fase 2.5: Ingenier√≠a de Caracter√≠sticas Avanzada - El Eslab√≥n Perdido
*Objetivo: Transformar el modelo de "n√∫meros crudos" a caracter√≠sticas que capturen la f√≠sica del sorteo.*

### üî¥ Variables de Recencia (Gaps) - CR√çTICO
- [x] **[IMP-FEAT-004] Vector de Gaps (Recencia)**: Crear un vector de tama√±o 41 (LOTO) donde cada posici√≥n sea el `lag` actual de ese n√∫mero ("hace cu√°ntos sorteos no sale el 5"). Esta es la variable m√°s predictiva en sistemas mec√°nicos (ley del retorno a la media).
- [x] **[IMP-FEAT-005] Inyectar Gaps en OraculoNeural**: A√±adir el vector de `Gaps` (Recencia) a `input_features` en `_preparar_dataset`.

### üü° Deltas y Velocidad
- [x] **[IMP-FEAT-006] Deltas Promedio**: Calcular la diferencia promedio entre los n√∫meros de los √∫ltimos 3 sorteos como feature adicional.

### üü° Meta-Features del Biom√©trico
- [x] **[IMP-FEAT-007] Inyectar Meta-Features**: El `generador_biometrico.py` calcula paridad y terminaciones pero no las pasa al modelo. Inyectar `paridad_promedio`, `suma_total`, y `terminacion_mas_frecuente` de la ventana anterior como columnas en X.

---

## üéØ Fase 2.6: Estrategia RACHA - Inversi√≥n del Problema (Negative Selection)
*Objetivo: Cambiar el enfoque de "predecir ganadores" a "descartar perdedores".*

El modelo `MultiOutputClassifier` para RACHA (20 n√∫meros, elegir 10) est√° condenado al 50% (azar puro) porque intenta minimizar el error cuadr√°tico medio, lo que lo lleva a predecir siempre el promedio.

### üî¥ Cambio de Arquitectura
- [x] **[IMP-RACHA-001] Clasificaci√≥n Binaria por N√∫mero**: Transformar el dataset de 1 fila por sorteo a **20 filas por sorteo** (una por cada bola posible).
  - *Features*: Recencia de la bola, Frecuencia en los √∫ltimos 10/50/100 sorteos, ¬øSali√≥ en el sorteo anterior?
  - *Target*: `1` (Sali√≥) o `0` (No sali√≥).
- [x] **[IMP-RACHA-002] Estrategia de Selecci√≥n Negativa**: Entrenar al modelo para encontrar los **0s m√°s seguros** (n√∫meros que *seguro* no saldr√°n) y descartarlos. Es matem√°ticamente m√°s f√°cil identificar una "bola fr√≠a" que una "bola caliente".

---

## ‚öôÔ∏è Fase 2.7: Ajuste de Hiperpar√°metros y Modelo
*Objetivo: Escapar del underfitting causado por configuraci√≥n demasiado conservadora.*

Tu GridSearch actual es demasiado conservador (`max_depth: 3-8`). Est√°s induciendo *underfitting* (sesgo alto) para evitar el *overfitting*.

### üü° Migraci√≥n a XGBoost/LightGBM
- [x] **[IMP-ML-009] Activar XGBoost en `_build_model`**: Veo el `try/import` en tu c√≥digo, pero el `_build_model` fuerza `RandomForest`. XGBoost maneja mejor los datos tabulares desbalanceados y valores nulos.
- [x] **[IMP-ML-010] Configurar XGBClassifier para RACHA**: Usar `objective='binary:logistic'` para la estrategia de RACHA transformada.

### üîµ Funci√≥n de Objetivo Personalizada
- [x] **[IMP-ML-011] M√©trica de Distancia Num√©rica**: El Random Forest optimiza "Accuracy" o "Gini". En loter√≠a, fallar por 1 n√∫mero (sacar 40 cuando sali√≥ 41) es un fallo total para el modelo, pero un "casi acierto" para la f√≠sica. Definir una m√©trica de evaluaci√≥n que penalice menos los errores cercanos (distancia num√©rica). *Implementado via `logloss` en XGBoost que penaliza proporcionalmente a la confianza del error.*

---

## üìä Fase 2.8: Validaci√≥n - La Ilusi√≥n del "Accuracy"
*Objetivo: Implementar m√©tricas que reflejen el valor real del modelo.*

Los logs muestran `Test Accuracy: 0.000` o `0.1041`. Esto es enga√±oso. En un espacio de (41 choose 6) combinaciones, el accuracy exacto siempre ser√° cercano a 0.

### üî¥ Nueva M√©trica de √âxito
- [x] **[IMP-VAL-001] Implementar "Hit Rate @ K"**: De los 10 n√∫meros que tu modelo predijo con mayor probabilidad (usando `predict_proba`), ¬øcu√°ntos estaban realmente en el sorteo ganador?
- [x] **[IMP-VAL-002] Optimizar para Top-K**: Si tu modelo consistentemente mete 1 o 2 n√∫meros ganadores en su Top 10 de probabilidades, ya tienes una ventaja sobre el azar. Optimiza para maximizar esa m√©trica, no el accuracy binario.

### üü° Validaci√≥n de Scraper
- [x] **[IMP-VAL-003] Validaci√≥n de Esquema JSON en Scraper**: A√±adir validaci√≥n de esquema JSON. Si `results` est√° vac√≠o, no guardar nada y lanzar error expl√≠cito para no entrenar con ceros.

---

## üõ†Ô∏è Fase 3: Arquitectura y Mantenibilidad (Mes 2)
*Objetivo: Pagar deuda t√©cnica y preparar el sistema para escalar.*

### üîµ Refactorizaci√≥n
- [ ] **[ERR-010] Centralizaci√≥n de Configuraci√≥n**: Mover todas las constantes (HORARIOS, GAME_CONFIG) a `config.py` y eliminar duplicados.
- [ ] **[ERR-013] Unificaci√≥n de Parsers**: Consolidar `loto_parser_v3.py` y `loto_parsers_mix.py` en un m√≥dulo √∫nico y robusto.
- [ ] **[ERR-017] Estandarizaci√≥n de Logging**: Reemplazar todos los `print()` dispersos por un sistema de `logging` estructurado y rotativo.

### üîµ Rendimiento
- [ ] **[ERR-011] Optimizaci√≥n Forense**: Reducir el loop de intentos en `predict_smart_gaussian` (de 5000 a ~200) para acelerar predicciones.

---

## üîÆ Fase 4: Expansi√≥n y Futuro (Mes 3+)
*Objetivo: Nuevas capacidades y mejoras de experiencia de usuario.*

### ‚ö™ Frontend y UX
- [ ] **[IMP-FE-001] Cach√© Local**: Implementar `localStorage` para CSVs en el frontend y reducir carga inicial.
- [ ] **Modo Oscuro Nativo**: Mejorar la experiencia visual en entornos con poca luz.
- [ ] **Vista M√≥vil**: Optimizar tablas y gr√°ficos para pantallas peque√±as.

### ‚ö™ I+D (Investigaci√≥n y Desarrollo)
- [ ] **[IMP-ML-004] Stacking Ensemble**: Crear un s√∫per-modelo que combine las predicciones de RF, XGBoost y modelos estad√≠sticos.
- [ ] **[IMP-FEAT-002] Embeddings de Combinaciones**: Experimentar con Word2Vec para encontrar relaciones sem√°nticas entre jugadas hist√≥ricas.
- [ ] **Migraci√≥n a SQL**: Mover de CSV a SQLite/PostgreSQL para manejo eficiente de millones de registros.
